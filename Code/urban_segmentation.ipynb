{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, lab_utils, random\n",
    "from torchvision.datasets import CIFAR10 \n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json, string\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a mofified VGG network with deconvolutional layers added.\n",
    "class vgg_deconv(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes=21, batch_norm = 0.1):\n",
    "        super(vgg_deconv, self).__init__()\n",
    "        \n",
    "        seg_vgg11 = models.vgg11(pretrained = True)\n",
    "        \n",
    "        # Convolutional layers.\n",
    "        self.conv1 = seg_vgg11.features[0]\n",
    "        self.bn1 = nn.BatchNorm2d(64,momentum=batch_norm)\n",
    "        self.conv2 = seg_vgg11.features[3]\n",
    "        self.bn2 = nn.BatchNorm2d(128,momentum=batch_norm)\n",
    "        self.conv3 = seg_vgg11.features[6]\n",
    "        self.bn3 = nn.BatchNorm2d(256,momentum=batch_norm)\n",
    "        self.conv4 = seg_vgg11.features[8]\n",
    "        self.bn4 = nn.BatchNorm2d(256,momentum=batch_norm)\n",
    "        self.conv5 = seg_vgg11.features[11]\n",
    "        self.bn5 = nn.BatchNorm2d(512,momentum=batch_norm)\n",
    "        self.conv6 = seg_vgg11.features[13]\n",
    "        self.bn6 = nn.BatchNorm2d(512,momentum=batch_norm)\n",
    "        self.conv7 = seg_vgg11.features[16]\n",
    "        self.bn7 = nn.BatchNorm2d(512,momentum=batch_norm)\n",
    "        self.conv8 = seg_vgg11.features[18]\n",
    "        self.bn8 = nn.BatchNorm2d(512,momentum=batch_norm)\n",
    "        \n",
    "        # Deconvolutional layers.\n",
    "        self.deconv8 = nn.ConvTranspose2d(512,512,3)\n",
    "        self.deconv_bn8 = nn.BatchNorm2d(512,momentum=batch_norm)\n",
    "        self.deconv7 = nn.ConvTranspose2d(512,512,3)\n",
    "        self.deconv_bn7 = nn.BatchNorm2d(512,momentum=batch_norm)\n",
    "        self.deconv6 = nn.ConvTranspose2d(512,512,3)\n",
    "        self.deconv_bn6 = nn.BatchNorm2d(512,momentum=batch_norm)\n",
    "        self.deconv5 = nn.ConvTranspose2d(512,256,3)\n",
    "        self.deconv_bn5 = nn.BatchNorm2d(256,momentum=batch_norm)\n",
    "        self.deconv4 = nn.ConvTranspose2d(256,256,3)\n",
    "        self.deconv_bn4 = nn.BatchNorm2d(256,momentum=batch_norm)\n",
    "        self.deconv3 = nn.ConvTranspose2d(256,128,3)\n",
    "        self.deconv_bn3 = nn.BatchNorm2d(128,momentum=batch_norm)\n",
    "        self.deconv2 = nn.ConvTranspose2d(128,64,3)\n",
    "        self.deconv_bn2 = nn.BatchNorm2d(64,momentum=batch_norm)\n",
    "        self.deconv1 = nn.ConvTranspose2d(64,n_classes,3)\n",
    "        self.deconv_bn1 = nn.BatchNorm2d(n_classes,momentum=batch_norm)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convolutional part.\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.max_pool2d(out,2,stride=2)\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = F.max_pool2d(out,2,stride=2)\n",
    "        out = F.relu(self.bn3(self.conv3(out)))\n",
    "        out = F.relu(self.bn4(self.conv4(out)))\n",
    "        out = F.max_pool2d(out,2,stride=2)\n",
    "        out = F.relu(self.bn5(self.conv5(out)))\n",
    "        out = F.relu(self.bn6(self.conv6(out)))\n",
    "        out = F.max_pool2d(out,2,stride=2)\n",
    "        out = F.relu(self.bn7(self.conv7(out)))\n",
    "        out = F.relu(self.bn8(self.conv8(out)))\n",
    "        \n",
    "        # Deconvolutional part.\n",
    "        out = F.relu(self.deconv_bn8(self.deconv8(out)))\n",
    "        out = F.relu(self.deconv_bn7(self.deconv7(out)))\n",
    "        out = F.max_unpool2d(out,2,stride=2)\n",
    "        out = F.relu(self.deconv_bn6(self.deconv6(out)))\n",
    "        out = F.relu(self.deconv_bn5(self.deconv5(out)))\n",
    "        out = F.max_unpool2d(out,2,stride=2)\n",
    "        out = F.relu(self.deconv_bn4(self.deconv4(out)))\n",
    "        out = F.relu(self.deconv_bn3(self.deconv3(out)))\n",
    "        out = F.max_unpool2d(out,2,stride=2)\n",
    "        out = F.relu(self.deconv_bn2(self.deconv2(out)))\n",
    "        out = F.max_unpool2d(out,2,stride=2)\n",
    "        out = F.relu(self.deconv_bn1(self.deconv1(out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "# These are used to record loss and accuracy data for further plots.\n",
    "loss_train = np.zeros(30)\n",
    "loss_validate = np.zeros(30)\n",
    "acc_train = np.zeros(30)\n",
    "acc_validate = np.zeros(30)\n",
    "\n",
    "# Define the training function.\n",
    "def train_model(network, criterion, optimizer, trainLoader, valLoader, n_epochs = 10, use_gpu = False):\n",
    "    if use_gpu:\n",
    "        #network = torch.nn.DataParallel(network, device_ids=[0, 1])\n",
    "        network = network.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "        \n",
    "    # Training loop.\n",
    "    for epoch in range(0, n_epochs):\n",
    "        correct = 0.0\n",
    "        cum_loss = 0.0\n",
    "        counter = 0\n",
    "        \n",
    "        gc.collect()\n",
    "\n",
    "        # Make a pass over the training data.\n",
    "        t = tqdm(trainLoader, desc = 'Training epoch %d' % epoch)\n",
    "        network.train()  # This is important to call before training!\n",
    "        for (i, (inputs, labels)) in enumerate(t):\n",
    "            \n",
    "            gc.collect()\n",
    "\n",
    "            # Wrap inputs, and targets into torch.autograd.Variable types.\n",
    "            inputs = Variable(inputs)\n",
    "            labels = Variable(labels)\n",
    "            \n",
    "            if use_gpu:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # Forward pass:\n",
    "            outputs = network(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass:\n",
    "            optimizer.zero_grad()\n",
    "            # Loss is a variable, and calling backward on a Variable will\n",
    "            # compute all the gradients that lead to that Variable taking on its\n",
    "            # current value.\n",
    "            loss.backward() \n",
    "\n",
    "            # Weight and bias updates.\n",
    "            optimizer.step()\n",
    "\n",
    "            # logging information.\n",
    "            cum_loss += loss.data[0]\n",
    "            max_scores, max_labels = outputs.data.max(1)\n",
    "            correct += (max_labels == labels.data).sum()\n",
    "            counter += inputs.size(0)\n",
    "            t.set_postfix(loss = cum_loss / (1 + i), accuracy = 100 * correct / counter)\n",
    "        \n",
    "        # Record data for plotting figures.\n",
    "        loss_train[epoch] = cum_loss / (1 + i)\n",
    "        acc_train[epoch] = 100 * correct / counter\n",
    "        \n",
    "        # Make a pass over the validation data.\n",
    "        correct = 0.0\n",
    "        cum_loss = 0.0\n",
    "        counter = 0\n",
    "        t = tqdm(valLoader, desc = 'Validation epoch %d' % epoch)\n",
    "        network.eval()  # This is important to call before evaluating!\n",
    "        for (i, (inputs, labels)) in enumerate(t):\n",
    "\n",
    "            # Wrap inputs, and targets into torch.autograd.Variable types.\n",
    "            inputs = Variable(inputs)\n",
    "            labels = Variable(labels)\n",
    "            \n",
    "            if use_gpu:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # Forward pass:\n",
    "            outputs = network(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # logging information.\n",
    "            cum_loss += loss.data[0]\n",
    "            max_scores, max_labels = outputs.data.max(1)\n",
    "            correct += (max_labels == labels.data).sum()\n",
    "            counter += inputs.size(0)\n",
    "            t.set_postfix(loss = cum_loss / (1 + i), accuracy = 100 * correct / counter)\n",
    "            \n",
    "        # Record data for plotting figures.\n",
    "        loss_validate[epoch] = cum_loss / (1 + i)\n",
    "        acc_validate[epoch] = 100 * correct / counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the neural network.\n",
    "learningRate = 0.01  # Feel free to change this.\n",
    "set_momentum = 0.5\n",
    "decay = 5e-3\n",
    "\n",
    "# Definition of our network.\n",
    "network = MyNetwork()\n",
    "\n",
    "# Feel free to use a different loss here.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Feel free to change the optimizer, or the optimizer parameters. e.g. momentum, weightDecay, etc.\n",
    "optimizer = optim.SGD(network.parameters(), lr = learningRate, momentum = set_momentum, weight_decay = decay)\n",
    "\n",
    "# Train the previously defined model.\n",
    "train_model(network, criterion, optimizer, trainLoader, valLoader, n_epochs = 30, use_gpu = True)\n",
    "\n",
    "# Plot loss and accuracy.\n",
    "ep = np.arange(1, 31, 1)\n",
    "plt.plot(ep, loss_train, 'ro',ep, loss_validate, 'bs')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('average loss')\n",
    "plt.legend(['training','validation'])\n",
    "plt.show()\n",
    "plt.plot(ep, acc_train, 'ro',ep, acc_validate, 'bs')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['training','validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG (\n",
      "  (features): Sequential (\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU (inplace)\n",
      "    (2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU (inplace)\n",
      "    (5): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU (inplace)\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU (inplace)\n",
      "    (10): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): ReLU (inplace)\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (14): ReLU (inplace)\n",
      "    (15): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU (inplace)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): ReLU (inplace)\n",
      "    (20): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  )\n",
      "  (classifier): Sequential (\n",
      "    (0): Linear (25088 -> 4096)\n",
      "    (1): ReLU (inplace)\n",
      "    (2): Dropout (p = 0.5)\n",
      "    (3): Linear (4096 -> 4096)\n",
      "    (4): ReLU (inplace)\n",
      "    (5): Dropout (p = 0.5)\n",
      "    (6): Linear (4096 -> 1000)\n",
      "  )\n",
      ")\n",
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "# Load and modify pre-trained VGG-11 and add deconvolutional layers.\n",
    "seg_vgg11 = models.vgg11(pretrained = True)\n",
    "\n",
    "print(seg_vgg11)\n",
    "\n",
    "conv11 = seg_vgg11.features[0]\n",
    "\n",
    "print(conv11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
